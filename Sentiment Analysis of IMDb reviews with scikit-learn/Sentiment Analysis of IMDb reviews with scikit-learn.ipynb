{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules are imported\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Yash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "print('Modules are imported')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  I recently bought the DVD, forgetting just how...          0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df\n",
    "movie_data = pd.read_csv(\"movie_data.csv\")\n",
    "movie_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming Documents into Feature Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer()\n",
    "docs = np.array([movie_data[\"review\"][1]])\n",
    "\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ok': 83, 'so': 103, 'really': 94, 'like': 62, 'kris': 59, 'kristofferson': 60, 'and': 5, 'his': 48, 'usual': 125, 'easy': 27, 'going': 38, 'delivery': 21, 'of': 82, 'lines': 63, 'in': 53, 'movies': 76, 'age': 3, 'has': 44, 'helped': 46, 'him': 47, 'with': 136, 'soft': 104, 'spoken': 107, 'low': 66, 'energy': 31, 'style': 110, 'he': 45, 'will': 135, 'steal': 108, 'scene': 99, 'effortlessly': 28, 'but': 12, 'disappearance': 23, 'is': 56, 'misstep': 71, 'holy': 50, 'moly': 72, 'this': 118, 'was': 129, 'bad': 7, 'movie': 75, 'br': 9, 'must': 77, 'give': 37, 'kudos': 61, 'to': 121, 'the': 116, 'cinematography': 16, 'actors': 1, 'including': 54, 'for': 34, 'trying': 122, 'their': 117, 'darndest': 20, 'make': 68, 'sense': 102, 'from': 35, 'goofy': 41, 'confusing': 19, 'story': 109, 'none': 80, 'it': 57, 'made': 67, 'probably': 92, 'didn': 22, 'understand': 123, 'either': 29, 'just': 58, 'through': 119, 'motions': 74, 'hoping': 51, 'someone': 106, 'would': 137, 'come': 17, 'up': 124, 'tell': 113, 'what': 132, 'all': 4, 'about': 0, 'don': 25, 'care': 13, 'that': 115, 'everyone': 33, 'on': 84, 'doing': 24, 'out': 87, 'love': 65, 'project': 93, 'or': 86, 'some': 105, 'such': 111, 'nonsense': 81, 've': 127, 'seen': 101, 'budget': 11, 'had': 43, 'plot': 91, 'goodness': 40, 'sake': 98, 'zilcho': 140, 'nada': 79, 'zippo': 141, 'empty': 30, 'reason': 95, 'complete': 18, 'waste': 130, 'good': 39, 'talent': 112, 'scenery': 100, 'celluloid': 15, 'rented': 96, 'piece': 89, 'garbage': 36, 'buck': 10, 'want': 128, 'my': 78, 'money': 73, 'back': 6, 'hours': 52, 'invested': 55, 'grade': 42, 'time': 120, 'watch': 131, 'minute': 70, 'your': 139, 'valuable': 126, 'while': 134, 'passing': 88, 'room': 97, 'where': 133, 'playing': 90, 'even': 32, 'open': 85, 'case': 14, 'holding': 49, 'dvd': 26, 'believe': 8, 'me': 69, 'you': 138, 'll': 64, 'thank': 114, 'advice': 2}\n"
     ]
    }
   ],
   "source": [
    "print((count.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 9 2 1 1 6 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 5 1\n",
      "  1 1 2 1 1 1 1 2 1 2 1 3 4 1 1 1 1 1 1 1 2 4 1 3 1 1 1 1 1 1 2 1 1 2 1 1\n",
      "  1 1 1 3 2 1 3 1 2 1 8 1 2 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 3 7 1 7 2 2 3 1 1 2 1 1 1 2 4 3 1 1 1 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04 0.04 0.04 0.04 0.04 0.37 0.08 0.04 0.04 0.25 0.04 0.04 0.04 0.04\n",
      "  0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.08 0.04 0.04\n",
      "  0.04 0.04 0.04 0.04 0.04 0.04 0.2  0.04 0.04 0.04 0.08 0.04 0.04 0.04\n",
      "  0.04 0.08 0.04 0.08 0.04 0.12 0.16 0.04 0.04 0.04 0.04 0.04 0.04 0.04\n",
      "  0.08 0.16 0.04 0.12 0.04 0.04 0.04 0.04 0.04 0.04 0.08 0.04 0.04 0.08\n",
      "  0.04 0.04 0.04 0.04 0.04 0.12 0.08 0.04 0.12 0.04 0.08 0.04 0.33 0.04\n",
      "  0.08 0.04 0.12 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04\n",
      "  0.04 0.04 0.04 0.04 0.08 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04\n",
      "  0.04 0.04 0.04 0.12 0.29 0.04 0.29 0.08 0.08 0.12 0.04 0.04 0.08 0.04\n",
      "  0.04 0.04 0.08 0.16 0.12 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04\n",
      "  0.04 0.04]]\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfTransformer(use_idf = True,\n",
    "                         norm = 'l2',\n",
    "                         smooth_idf= True)\n",
    "np.set_printoptions(precision=2)\n",
    "print(tfidf.fit_transform(bag).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticon = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+', \" \", text.lower()) +\\\n",
    "    \" \".join(emoticon).replace('-', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ok so i really like kris kristofferson and his usual easy going delivery of lines in his movies age has helped him with his soft spoken low energy style and he will steal a scene effortlessly but disappearance is his misstep holy moly this was a bad movie i must give kudos to the cinematography and and the actors including kris for trying their darndest to make sense from this goofy confusing story none of it made sense and kris probably didn t understand it either and he was just going through the motions hoping someone would come up to him and tell him what it was all about i don t care that everyone on this movie was doing out of love for the project or some such nonsense i ve seen low budget movies that had a plot for goodness sake this had none zilcho nada zippo empty of reason a complete waste of good talent scenery and celluloid i rented this piece of garbage for a buck and i want my money back i want my 2 hours back i invested on this grade f waste of my time don t watch this movie or waste 1 minute of your valuable time while passing through a room where it s playing or even open up the case that is holding the dvd believe me you ll thank me for the advice '"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(movie_data[\"review\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'blah blah blah :) :('"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(\"blah blah :-) :( :-| blah </p>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data = movie_data[\"review\"].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "## okenisation of textt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'see', 'seashells', 'at', 'a', 'sea', 'which', 'has', 'a', 'shape', 'c']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"I see seashells at a sea which has a shape c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'see', 'seashel', 'at', 'a', 'sea', 'which', 'ha', 'a', 'shape', 'c']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_porter(\"I see seashells at a sea which has a shape c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'see', 'seashel', 'sea', 'ha', 'shape', 'c']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "[w for w in tokenizer_porter(\"I see seashells at a sea which has a shape c\")if w not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform text into data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents= None,\n",
    "                       lowercase= None,\n",
    "                       tokenizer=tokenizer_porter,\n",
    "                       use_idf=True,\n",
    "                       norm = 'l2',\n",
    "                       smooth_idf=True)\n",
    "y = movie_data[\"sentiment\"].values\n",
    "x = tfidf.fit_transform(movie_data[\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state = 0, test_size = 0.5, shuffle = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  5.6min remaining:  8.4min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  6.0min finished\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegressionCV(cv = 5,\n",
    "                          scoring = 'accuracy',\n",
    "                          random_state = 0,\n",
    "                          n_jobs = -1,\n",
    "                          verbose = 3, max_iter = 300).fit(x_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = open('saved.sav', 'wb')\n",
    "pickle.dump(clf, saved_model)\n",
    "saved_model.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'saved_model.sav'\n",
    "saved_clf = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89208"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = saved_clf.score(x_test, y_test)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model is 89.208%\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of the model is \", acc * 100, \"%\", sep=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
